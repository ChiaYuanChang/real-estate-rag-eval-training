# üìå Task 2 ‚Äî Evaluation Framework for VLM Tags Quality

## Task Description

### üéØ Objective

Given that VLM has produced descriptions and tags (or tag-like content), design an evaluation framework to quantify **tag
quality**.

Your evaluation must detect whether tags are:

- Correct / incorrect
- Too generic / too specific
- Redundant
- Missing critical spatial signals
- Hallucinated

---

### 1) Quantitative Metrics (Must Implement ‚â• 3)

You must implement at least **three** of the following metrics (more is welcome):

**(A) Image-Tag Alignment (CLIP Consistency)**
Compute CLIP similarity between each tag and the representative images.

- Aggregate: tag-level mean similarity, property-level alignment score.
- Penalize tags with low similarity across all images.

**(B) Redundancy / Overlap**
Compute embedding similarity among tags.

- Redundancy score: how many tags collapse into near-duplicates.

**(C) Specificity / Informativeness**
Possible heuristics: tag length, noun-phrase richness, or IDF-like scoring across corpus.

- Penalize vague tags like: ‚Äúnice‚Äù, ‚Äúcozy‚Äù, ‚Äúgood‚Äù.

**(D) Coverage**
Use VLM to extract ‚Äúvisual facts‚Äù from images and check whether tags cover them. Or use object/attribute detectors.

**(E) Self-Consistency Cross-Check (VLM-as-a-Judge)**
Ask a VLM: *‚ÄúGiven these images, is tag X correct? (yes/no) confidence‚Äù*. Aggregate correctness score.

---

### 2) Improvement Proposals (Must Be Actionable)

Based on metric results, propose next steps:

- Better prompting
- Tag normalization dictionary
- Per-room tagging then property aggregation

---

## 1.1 Problem Definition

The primary goal was to evaluate the quality of tags generated by a Vision Language Model (VLM). However, before evaluation
could begin, a critical data integrity issue was identified:

1. **Data Mismatch:** The number of picture URLs did not match the number of descriptions in the source dataset.
2. **Disordered Sequence:** The order of images and descriptions was randomized, making direct 1-to-1 mapping impossible.
3. **Missing Context:** Some raw descriptions were filtered out during previous data cleaning (for Neo4j ingestion), requiring
   data enrichment before evaluation.

Therefore, the pipeline required a robust **Data Re-matching** phase before the **Evaluation** phase could proceed.

## 1.2 Design Choices and Trade-offs

### Phase 1: Data Re-matching Strategy

| Approach                 | Method                                                                           | Outcome        | Reasoning                                                                                                                                                                                                                                                                         |
|:-------------------------|:---------------------------------------------------------------------------------|:---------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Attempt 1: CV / CLIP** | CLIP Encoding + Similarity Matrix + Hungarian Algorithm (Linear Sum Assignment). | ‚ùå **Failed**   | **Accuracy Issues:** The model struggled with semantic nuances in real estate. For example, it frequently misclassified a "Lobby" (ËøéË≥ìÂ§ßÂª≥) as a "Living Room" (ÂÆ¢Âª≥) due to visual similarities.                                                                                      |
| **Attempt 2: VLM (LLM)** | Using VLM to visually analyze and match images to text.                          | ‚úÖ **Selected** | **High Accuracy:** VLM understands context better than simple embeddings. <br>**Cost Optimization:** Initially used `gemini-1.5-pro` (excellent but expensive). Downgraded to `gemini-1.5-flash` (currently 2.5 flash), which maintained high accuracy at a fraction of the cost. |

### Phase 2: Evaluation Strategy (VLM-as-a-Judge)

Instead of relying solely on traditional NLP metrics (TF-IDF/LDA), which required extensive normalization logic that was out of
scope for the timeline, we adopted a **VLM-as-a-Judge** approach.

* **Why:** This covers metrics **(C) Specificity**, **(D) Coverage**, and **(E) Self-Consistency** in a single pass.
* **Implementation:** We designed prompts that force the model to output confidence scores and assess hallucinations, vagueness,
  and bias.
* **Enhancement:** A secondary evaluation pipeline was created incorporating **Spatial Depth Maps** to specifically judge the
  objectivity of spatial signals.

## 1.3 System Architecture

The system follows a pipeline from raw data cleaning, through re-matching, to final evaluation.

```mermaid
flowchart TD
    subgraph Data_Prep [Data Preparation & Re-matching]
        A[Raw Data: cleaned_twhg_with_latlng_and_places] -->|Input| B{Matching Strategy}
        B -->|Attempt 1: CLIP + Hungarian| C[Script: clip_hungarian_property_aligner.py]
        C -.->|Low Accuracy| D[Discarded]
        B -->|Attempt 2: VLM Gemini Flash| E[Script: vlm_tag_data_rematching.py]
        E -->|Output| F[Matched Data: vlm_rematch_twhg...]
        F -->|Missing Info?| G[Script: add_raw_description.py]
        G -->|Enrichment| H[Final Dataset: vlm_rematch_add_info_twhg...]
    end

subgraph Evaluation [VLM-as-a-Judge Evaluation]
H --> I{Eval Mode}

I -->|Standard Judge|J[Script: vlm_as_a_judge_evalution.py]
J -->|Prompting|K[Assess: Hallucination, Specificity, Coverage]
K --> L[Output: data/vlm_as_a_judge/]

I -->|Spatial Judge|M[Script: vlm_as_a_judge_evalution.py]
M -->|Input + Depth Maps|N[Assess: Spatial Objectivity & Depth]
N --> O[Output: data/vlm_with_spatial_signals_info_as_a_judge/]
end

subgraph NLP_Analysis [Traditional NLP (Paused)]
H -.-> P[Script: raw_data_analyze/]
P -.-> Q[TF-IDF / LDA / Room Clustering]
Q -.->|Time Constraint|R[On Hold]
end
```

## 1.4 Implementation Steps & Scripts

To reproduce the results, follow this execution order:

### Step 1: Dataset Re-matching

Fix the mismatch between image URLs and descriptions.

* **Script:** `scripts/vlm_tag_quality_service/vlm_tag_data_rematching.py`
* **Model:** `gemini-1.5-flash`
* **Input:** `data/cleaned_twhg_with_latlng_and_places/`
* **Output:** `data/vlm_rematch_twhg_with_latlng_and_places/`

### Step 2: Data Enrichment

Restore raw descriptions filtered out during previous Neo4j cleaning steps.

* **Script:** `scripts/vlm_tag_quality_service/add_raw_description.py`
* **Input:** `data/vlm_rematch_twhg_with_latlng_and_places/`
* **Output:** `data/vlm_rematch_add_info_twhg_with_latlng_and_places/`

### Step 3: Evaluation (VLM-as-a-Judge)

Run the evaluation using a larger model to judge the tags generated by smaller models.

* **Service Logic:** `services/vlm_tag_quality_service/__init__.py`
* **Execution Script:** `scripts/vlm_tag_quality_service/vlm_as_a_judge_evalution.py`
* **Modes:**
    1. **Standard Eval:** Checks for hallucinations and generic tags.
        * *Output:* `data/vlm_tag_quality_service/vlm_as_a_judge/`
    2. **Spatial Eval:** Includes relative depth maps to check spatial signal accuracy.
        * *Output:* `data/vlm_tag_quality_service/vlm_with_spatial_signals_info_as_a_judge/`

*(Note: The failed CLIP approach script is located at `scripts/vlm_tag_quality_service/clip_hungarian_property_aligner.py` for
reference only.)*

## 1.5 Challenges and Obstacles

1. **CLIP Limitations:** The initial attempt to use CLIP embeddings with Linear Sum Assignment failed because the model could
   not distinguish between semantically similar but functionally different spaces (e.g., Lobby vs. Living Room).
2. **Data Loss:** Critical raw descriptions were missing from the cleaned dataset used for Neo4j, necessitating an extra
   enrichment step (`add_raw_description.py`).
3. **NLP Complexity:** An attempt was made to use traditional NLP (TF-IDF, LDA) to cluster room names (Room, Hall, Bath, Other)
   and normalize scores. This was paused due to the complexity of normalization logic and time constraints, favoring the
   VLM-as-a-Judge approach.

## 1.6 Future Outlook & Recommendations

Based on the evaluation results and the architectural journey, the following improvements are proposed for the next iteration:

### 1. Schema & Data Structure Redesign

* **Pre-binding:** Instead of re-matching later, the `image_url`, `description`, and `room_name` should be designed as a single
  cohesive Object from the start of the pipeline.
* **Strict Typing:** Implement `list[strEnum]` for `room_name` (e.g., `LIVING_ROOM`, `DINING_ROOM`, `COMBINED_LIVING_DINING`) to
  enforce consistency.
* **Reasoning Field:** Add a `reason` field to the schema to capture the raw nuance that a strict Enum might miss.

### 2. Scenario-Based Tagging

* **Breakdown:** Use smaller, specialized VLMs for specific scenarios rather than one giant prompt.
* **Explicit Attributes:** Extract critical decision-making metrics into dedicated fields rather than generic tags:
    * **Style:** (Modern, Industrial, etc.)
    * **Ventilation:** (Good, Poor, Windowless)
    * **Lighting:** (Natural light, Dark)
* **Usage:** These specific attributes are vital for user decision-making and should be generated during the initial tagging
  pipeline, serving as structured filters rather than fuzzy search tags.